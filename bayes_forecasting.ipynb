{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ee55163-318f-4055-bde7-255eb08c8c18",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b0ca035c-8a05-4762-9d41-ad9f3fea7d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e27b31a5ab6c46f8b6078bc4e1e33105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating val dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae6d0b8a66da4e47996ac8acc2fdaa9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d2e4bf4d534591b95805cdde07c5fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from climate_learn.utils.datetime import Year, Days, Hours\n",
    "from climate_learn.data import DataModule\n",
    "\n",
    "dm = DataModule(\n",
    "    dataset=\"ERA5\",\n",
    "    task=\"forecasting\",\n",
    "    root_dir=\"../climate-learn/data/weatherbench/era5/5.625\",\n",
    "    in_vars=[\"2m_temperature\"],\n",
    "    out_vars=[\"2m_temperature\"],\n",
    "    train_start_year=Year(1979),\n",
    "    val_start_year=Year(1980),\n",
    "    test_start_year=Year(1981),\n",
    "    end_year=Year(1982),\n",
    "    pred_range=Days(3),\n",
    "    subsample=Hours(6),\n",
    "    batch_size=128,\n",
    "    num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c4040a-19f6-46f3-921c-1a54c5a0c315",
   "metadata": {},
   "source": [
    "Create BayesianCNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6d8c6d44-1c2a-4766-828b-26e2243554fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 32, 64])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2a9381-c48e-4e4d-956f-9a68d2764793",
   "metadata": {},
   "source": [
    "# Build BayesCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "6dfa7463-d8d9-410f-9ad0-b6b82d461899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.0 59.0\n"
     ]
    }
   ],
   "source": [
    "# Scratch cell for computing output sizes\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "hout = np.floor((29 + 2 * 0 - 1 * (3 - 1) - 1) / 1 + 1)\n",
    "wout = np.floor((61 + 2 * 0 - 1 * (3 - 1) - 1) / 1 + 1)\n",
    "\n",
    "print(hout, wout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "78e3cc64-6201-44a1-8e5f-b719c0ed763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import *\n",
    "import torch.nn as nn\n",
    "\n",
    "# https://arxiv.org/pdf/1901.02731.pdf\n",
    "\n",
    "class BayesCNN(ModuleWrapper):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = BBB_Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=3,\n",
    "            kernel_size=5,\n",
    "            stride=1,\n",
    "            padding=2\n",
    "        )\n",
    "        self.act1 = nn.Softplus()\n",
    "        self.conv2 = BBB_Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=5,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        # self.maxpool2 = nn.AvgPool2d(2)\n",
    "        # self.bn2 = nn.BatchNorm2d(5)\n",
    "        # self.tanh2 = nn.Tanh()\n",
    "        # self.conv3 = BBB_Conv2d(\n",
    "        #     in_channels=5,\n",
    "        #     out_channels=7,\n",
    "        #     kernel_size=3,\n",
    "        #     stride=1\n",
    "        # )\n",
    "        # self.bn3 = nn.BatchNorm2d(7)\n",
    "        # self.tanh3 = nn.Tanh()\n",
    "        # self.conv4 = BBB_Conv2d(\n",
    "        #     in_channels=7,\n",
    "        #     out_channels=7,\n",
    "        #     kernel_size=3,\n",
    "        #     stride=1\n",
    "        # )\n",
    "        # self.flatten = FlattenLayer(2352)\n",
    "        # self.fc = BBB_Linear(2352, 32*64)\n",
    "        self.act2 = nn.Softplus()\n",
    "        self.conv3 = BBB_Conv2d(\n",
    "            in_channels=5,\n",
    "            out_channels=3,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "    \n",
    "    # def forward(self, x):\n",
    "    #     orig_shape = x.shape\n",
    "    #     x = self.conv1(x)\n",
    "    #     x = self.bn1(x)\n",
    "    #     x = self.tanh1(x)\n",
    "    #     x = self.conv2(x)\n",
    "    #     x = self.maxpool2(x)\n",
    "    #     x = self.bn2(x)\n",
    "    #     x = self.tanh2(x)\n",
    "    #     x = self.conv3(x)\n",
    "    #     x = self.bn3(x)\n",
    "    #     x = self.tanh3(x)\n",
    "    #     x = self.conv4(x)\n",
    "    #     x = self.flatten(x)\n",
    "    #     x = self.fc(x)\n",
    "    #     z = x.reshape(orig_shape)\n",
    "    #     return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "86b2d081-41ab-42f4-a567-d6ab7cb7ed01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "aed6615b-0e74-465a-818b-d0da63aad583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test it\n",
    "# outputs are (logits, kl)\n",
    "net = BayesCNN().cuda()\n",
    "logits, _ = net(dm.train_dataset[0][0].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "a9baa1f9-7a47-49f0-9a8b-374ea4e870c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 6.7370e-02, -1.2802e-01, -6.8495e-03,  ...,  2.5561e-03,\n",
       "            8.8802e-03, -5.3915e-02],\n",
       "          [ 2.1930e-01, -1.0504e-02,  6.8589e-02,  ...,  5.9458e-02,\n",
       "            4.7761e-02,  7.1697e-03],\n",
       "          [ 2.6401e-01,  5.7176e-02,  1.3267e-01,  ...,  7.8685e-02,\n",
       "            7.5449e-02, -2.7352e-02],\n",
       "          ...,\n",
       "          [ 2.4301e-01,  4.5493e-05,  8.4145e-02,  ...,  9.2749e-02,\n",
       "            1.0889e-01,  6.9901e-02],\n",
       "          [ 2.1565e-01, -1.6570e-02,  4.6437e-02,  ..., -9.4138e-03,\n",
       "           -7.0095e-03, -5.5375e-02],\n",
       "          [ 1.3910e-01,  4.5960e-02,  6.4850e-02,  ...,  4.1872e-02,\n",
       "            3.8163e-02, -7.2926e-02]],\n",
       "\n",
       "         [[-2.1275e-01, -5.7509e-02, -5.8342e-02,  ..., -3.0651e-02,\n",
       "           -3.0758e-02,  6.0147e-02],\n",
       "          [-1.9617e-01, -2.2463e-02, -3.5384e-02,  ..., -1.0935e-02,\n",
       "            8.1864e-02,  4.8895e-02],\n",
       "          [-1.8487e-01, -3.0218e-02, -4.0427e-02,  ..., -5.7251e-02,\n",
       "            1.4066e-02,  4.0066e-02],\n",
       "          ...,\n",
       "          [-1.8779e-01, -6.2414e-02, -1.0434e-01,  ..., -1.0332e-01,\n",
       "           -2.4712e-02,  1.8111e-02],\n",
       "          [-1.8437e-01, -5.2070e-02, -7.2846e-02,  ..., -6.6345e-02,\n",
       "            3.4625e-02, -1.1148e-02],\n",
       "          [-1.7617e-01,  8.8379e-02,  3.8316e-02,  ...,  2.0426e-02,\n",
       "            1.3418e-01,  3.9849e-02]],\n",
       "\n",
       "         [[-2.9166e-01, -6.2964e-01, -6.0662e-01,  ..., -6.2655e-01,\n",
       "           -6.9542e-01, -2.3081e-01],\n",
       "          [-1.8218e-01, -3.9719e-01, -3.2330e-01,  ..., -3.6458e-01,\n",
       "           -3.9127e-01,  8.7316e-02],\n",
       "          [-2.3088e-01, -4.8532e-01, -4.1725e-01,  ..., -3.9214e-01,\n",
       "           -3.8994e-01,  1.1364e-01],\n",
       "          ...,\n",
       "          [-1.7603e-01, -4.2343e-01, -3.7460e-01,  ..., -3.7174e-01,\n",
       "           -4.0428e-01,  1.3116e-01],\n",
       "          [-2.1105e-01, -4.5233e-01, -3.7424e-01,  ..., -3.5254e-01,\n",
       "           -3.3046e-01,  1.4393e-01],\n",
       "          [-1.9023e-01, -3.3462e-01, -2.6815e-01,  ..., -2.7705e-01,\n",
       "           -2.1916e-01,  6.8899e-02]]]], device='cuda:0',\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "4dbed5e8-768e-4a91-85f4-98c2b45499f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 32, 64])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.detach().cpu().size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "47764f31-cf3a-4c17-8a2e-465d34bde31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torchvision.transforms import transforms\n",
    "from climate_learn.models.modules.utils.metrics import (\n",
    "    lat_weighted_mse,\n",
    "    lat_weighted_rmse,\n",
    "    lat_weighted_acc,    \n",
    ")\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "class LitModule(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = BayesCNN()\n",
    "        self.train_loss = [lat_weighted_mse]\n",
    "        self.val_loss = [\n",
    "            lat_weighted_rmse,\n",
    "            lat_weighted_acc\n",
    "        ]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # if using the BayesCNN pre-defined forward\n",
    "        logits, _ = self.net(torch.squeeze(x, 1))\n",
    "        # otherwise\n",
    "        # logits = self.net(torch.squeeze(x, 1))\n",
    "        return logits\n",
    "    \n",
    "    def set_denormalization(self, mean, std):\n",
    "        self.denormalization = transforms.Normalize(mean, std)\n",
    "\n",
    "        mean_mean_denorm, mean_std_denorm = -mean / std, 1 / std\n",
    "        self.mean_denormalize = transforms.Normalize(mean_mean_denorm, mean_std_denorm)\n",
    "\n",
    "        std_mean_denorm, std_std_denorm = np.zeros_like(std), 1 / std\n",
    "        self.std_denormalize = transforms.Normalize(std_mean_denorm, std_std_denorm)\n",
    "\n",
    "        mean_mean_denorm, mean_std_denorm = -mean / std, 1 / std\n",
    "        self.mean_denormalize = transforms.Normalize(mean_mean_denorm, mean_std_denorm)\n",
    "\n",
    "        std_mean_denorm, std_std_denorm = np.zeros_like(std), 1 / std\n",
    "        self.std_denormalize = transforms.Normalize(std_mean_denorm, std_std_denorm)\n",
    "\n",
    "        mean_mean_denorm, mean_std_denorm = -mean / std, 1 / std\n",
    "        self.mean_denormalize = transforms.Normalize(mean_mean_denorm, mean_std_denorm)\n",
    "\n",
    "        std_mean_denorm, std_std_denorm = np.zeros_like(std), 1 / std\n",
    "        self.std_denormalize = transforms.Normalize(std_mean_denorm, std_std_denorm)\n",
    "\n",
    "    def set_lat_lon(self, lat, lon):\n",
    "        self.lat = lat\n",
    "        self.lon = lon\n",
    "\n",
    "    def set_pred_range(self, r):\n",
    "        self.pred_range = r\n",
    "\n",
    "    def set_train_climatology(self, clim):\n",
    "        self.train_clim = clim\n",
    "\n",
    "    def set_val_climatology(self, clim):\n",
    "        self.val_clim = clim\n",
    "\n",
    "    def set_test_climatology(self, clim):\n",
    "        self.test_clim = clim\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, _, out_variables = batch\n",
    "        y_hat = self(x)\n",
    "        loss_dict = [\n",
    "            m(y_hat, y, out_variables, lat=self.lat)\n",
    "            for m in self.train_loss\n",
    "        ][0]\n",
    "        for var in loss_dict.keys():\n",
    "            self.log(\n",
    "                \"train/\" + var,\n",
    "                loss_dict[var],\n",
    "                on_step=True,\n",
    "                on_epoch=False,\n",
    "                prog_bar=True,\n",
    "                batch_size=len(x)\n",
    "            )\n",
    "        return loss_dict\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y, variables, out_variables = batch\n",
    "        pred_steps = y.shape[1]\n",
    "        pred_range = self.pred_range.hours()\n",
    "        \n",
    "        default_days = [1, 3, 5]\n",
    "        days_each_step = pred_range / 24\n",
    "        default_steps = [\n",
    "            d / days_each_step for d in default_days if d % days_each_step == 0\n",
    "        ]\n",
    "        steps = [int(s) for s in default_steps if s <= pred_steps and s > 0]\n",
    "        days = [int(s * pred_range / 24) for s in steps]\n",
    "        day = int(days_each_step)\n",
    "                \n",
    "        preds = []\n",
    "        for _ in range(pred_steps):\n",
    "            x = self.forward(x)\n",
    "            preds.append(x)\n",
    "        preds = torch.stack(preds, dim=1)\n",
    "        if len(y.shape) == 4:\n",
    "            y = y.unsqueeze(1)\n",
    "        loss_dict = [\n",
    "            m(preds, y, out_variables, transform=self.denormalization, lat=self.lat,\n",
    "              log_steps=steps, log_days=days, clim=self.val_clim)\n",
    "            for m in self.val_loss\n",
    "        ][0]\n",
    "        for var in loss_dict.keys():\n",
    "            self.log(\n",
    "                \"val/\" + var,\n",
    "                loss_dict[var],\n",
    "                on_step=True,\n",
    "                on_epoch=False,\n",
    "                prog_bar=True,\n",
    "                batch_size=len(x)\n",
    "            )\n",
    "        return loss_dict\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y, variables, out_variables = batch\n",
    "        pred_steps = y.shape[1]\n",
    "        pred_range = self.pred_range.hours()\n",
    "        day = int(pred_range / 24)\n",
    "        \n",
    "        default_days = [1, 3, 5]\n",
    "        days_each_step = pred_range / 24\n",
    "        default_steps = [\n",
    "            d / days_each_step for d in default_days if d % days_each_step == 0\n",
    "        ]\n",
    "        steps = [int(s) for s in default_steps if s <= pred_steps and s > 0]\n",
    "        days = [int(s * pred_range / 24) for s in steps]\n",
    "        day = int(days_each_step)\n",
    "        \n",
    "        # rmse for climatology baseline\n",
    "        clim_pred = self.train_clim  # C, H, W\n",
    "        clim_pred = (\n",
    "            clim_pred.unsqueeze(0)\n",
    "            .unsqueeze(0)\n",
    "            .repeat(y.shape[0], y.shape[1], 1, 1, 1)\n",
    "            .to(y.device)\n",
    "        )\n",
    "        baseline_rmse = lat_weighted_rmse(\n",
    "            clim_pred,\n",
    "            y,\n",
    "            out_variables,\n",
    "            transform_pred=False,\n",
    "            transform=self.denormalization,\n",
    "            lat=self.lat,\n",
    "            log_steps=steps,\n",
    "            log_days=days,\n",
    "        )\n",
    "        for var in baseline_rmse.keys():\n",
    "            self.log(\n",
    "                \"test_climatology_baseline/\" + var,\n",
    "                baseline_rmse[var],\n",
    "                on_step=False,\n",
    "                on_epoch=True,\n",
    "                sync_dist=True,\n",
    "                batch_size=len(x),\n",
    "            )\n",
    "\n",
    "        # rmse for persistence baseline\n",
    "        pers_pred = x  # B, 1, C, H, W\n",
    "        baseline_rmse = lat_weighted_rmse(\n",
    "            pers_pred,\n",
    "            y,\n",
    "            out_variables,\n",
    "            transform_pred=True,\n",
    "            transform=self.denormalization,\n",
    "            lat=self.lat,\n",
    "            log_steps=steps,\n",
    "            log_days=days,\n",
    "        )\n",
    "        for var in baseline_rmse.keys():\n",
    "            self.log(\n",
    "                \"test_persistence_baseline/\" + var,\n",
    "                baseline_rmse[var],\n",
    "                on_step=False,\n",
    "                on_epoch=True,\n",
    "                sync_dist=True,\n",
    "                batch_size=len(x),\n",
    "            )\n",
    "\n",
    "        # rmse for linear regression baseline\n",
    "        # check if fit_lin_reg_baseline is called by checking whether self.lr_baseline is initialized\n",
    "        try:\n",
    "            lr_pred = self.lr_baseline.predict(\n",
    "                x.cpu().reshape((x.shape[0], -1))\n",
    "            ).reshape(y.shape)\n",
    "        except AttributeError as e:\n",
    "            raise NotImplementedError(\n",
    "                \"Expect climate_learn.models.fit_lin_reg_baseline be implemented before test steps.\"\n",
    "            ) from None\n",
    "\n",
    "        lr_pred = lr_pred[:, np.newaxis, :, :, :]  # B, 1, C, H, W\n",
    "        lr_pred = torch.from_numpy(lr_pred).float().to(y.device)\n",
    "        baseline_rmse = lat_weighted_rmse(\n",
    "            lr_pred,\n",
    "            y,\n",
    "            out_variables,\n",
    "            transform_pred=True,\n",
    "            transform=self.denormalization,\n",
    "            lat=self.lat,\n",
    "            log_steps=steps,\n",
    "            log_days=days,\n",
    "        )\n",
    "        for var in baseline_rmse.keys():\n",
    "            self.log(\n",
    "                \"test_ridge_regression_baseline/\" + var,\n",
    "                baseline_rmse[var],\n",
    "                on_step=False,\n",
    "                on_epoch=True,\n",
    "                sync_dist=True,\n",
    "                batch_size=len(x),\n",
    "            )\n",
    "            \n",
    "        preds = []\n",
    "        for _ in range(pred_steps):\n",
    "            x = self.forward(x)\n",
    "            preds.append(x)\n",
    "        preds = torch.stack(preds, dim=1)\n",
    "        if len(y.shape) == 4:\n",
    "            y = y.unsqueeze(1)\n",
    "        loss_dict = [\n",
    "            m(preds, y, out_variables, transform=self.denormalization, lat=self.lat,\n",
    "              log_steps=steps, log_days=days, clim=self.test_clim)\n",
    "            for m in self.val_loss\n",
    "        ][0]\n",
    "        for var in loss_dict.keys():\n",
    "            self.log(\n",
    "                \"test/\" + var,\n",
    "                loss_dict[var],\n",
    "                on_step=False,\n",
    "                on_epoch=True,\n",
    "                prog_bar=True,\n",
    "                batch_size=len(x)\n",
    "            )\n",
    "        return loss_dict\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "    \n",
    "    def fit_lin_reg_baseline(self, train_dataset, reg_hparam=0.0):\n",
    "        X_train = train_dataset.inp_data.reshape(train_dataset.inp_data.shape[0], -1)\n",
    "        y_train = train_dataset.out_data.reshape(train_dataset.out_data.shape[0], -1)\n",
    "        self.lr_baseline = Ridge(alpha=reg_hparam)\n",
    "        self.lr_baseline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "d95831f2-9222-4012-9b32-a21c257bf50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LitModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "ab2875fa-beba-468f-8374-4941e99ef51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from climate_learn.models import set_climatology\n",
    "set_climatology(lm, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "3e2601db-e4d1-4d29-b44e-22122a42da22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "from climate_learn.training import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    seed=0,\n",
    "    accelerator=\"gpu\",\n",
    "    precision=16,\n",
    "    max_epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "6da9d28c-37c2-4649-b826-e58e63ae0e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/home/jason.jewik/miniconda3/envs/bayes/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/jason.jewik/PyTorch-BayesianCNN/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type      </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ net              │ BayesCNN  │    712 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ net.conv1        │ BBBConv2d │    156 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ net.act1         │ Softplus  │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ net.conv2        │ BBBConv2d │    280 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ net.act2         │ Softplus  │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ net.conv3        │ BBBConv2d │    276 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>│ denormalization  │ Normalize │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7 </span>│ mean_denormalize │ Normalize │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8 </span>│ std_denormalize  │ Normalize │      0 │\n",
       "└───┴──────────────────┴───────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType     \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ net              │ BayesCNN  │    712 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ net.conv1        │ BBBConv2d │    156 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ net.act1         │ Softplus  │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ net.conv2        │ BBBConv2d │    280 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ net.act2         │ Softplus  │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ net.conv3        │ BBBConv2d │    276 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m6\u001b[0m\u001b[2m \u001b[0m│ denormalization  │ Normalize │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m7\u001b[0m\u001b[2m \u001b[0m│ mean_denormalize │ Normalize │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m8\u001b[0m\u001b[2m \u001b[0m│ std_denormalize  │ Normalize │      0 │\n",
       "└───┴──────────────────┴───────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 712                                                                                              \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 712                                                                                                  \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 712                                                                                              \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 712                                                                                                  \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13812c09a1c41678088ca2722434d1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(lm, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "1e25e069-2f56-4590-ab0e-ac2175bc99a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.fit_lin_reg_baseline(dm.train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "0219b951-013f-4389-8540-da2117c6db5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab33add61b45415f8b5114231b3c885c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">                      Test metric                       </span>┃<span style=\"font-weight: bold\">                      DataLoader 0                      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">            test/w_rmse_2m_temperature_day_3            </span>│<span style=\"color: #800080; text-decoration-color: #800080\">                   7.390388445670543                    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> test_climatology_baseline/w_rmse_2m_temperature_day_3  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">                   5.883646065011019                    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> test_persistence_baseline/w_rmse_2m_temperature_day_3  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">                   3.1876519642077183                   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> test_ridge_regression_baseline/w_rmse_2m_temperature_… </span>│<span style=\"color: #800080; text-decoration-color: #800080\">                   3.3875521518582556                   </span>│\n",
       "└────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m                     Test metric                      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                     DataLoader 0                     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m           test/w_rmse_2m_temperature_day_3           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m                  7.390388445670543                   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtest_climatology_baseline/w_rmse_2m_temperature_day_3 \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m                  5.883646065011019                   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtest_persistence_baseline/w_rmse_2m_temperature_day_3 \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m                  3.1876519642077183                  \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtest_ridge_regression_baseline/w_rmse_2m_temperature_…\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m                  3.3875521518582556                  \u001b[0m\u001b[35m \u001b[0m│\n",
       "└────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.test(lm, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69393180-8f02-456f-9e48-df3b7e13d91a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayes",
   "language": "python",
   "name": "bayes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
