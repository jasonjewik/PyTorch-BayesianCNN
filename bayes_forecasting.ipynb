{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ee55163-318f-4055-bde7-255eb08c8c18",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0ca035c-8a05-4762-9d41-ad9f3fea7d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f29624cac90149fdadd2f7e88d6a4724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating val dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ad51395f874fd9ae2a0ff888725afd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f93a2beba0247d3a82e41ceb701996f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from climate_learn.utils.datetime import Year, Days, Hours\n",
    "from climate_learn.data import DataModule\n",
    "\n",
    "dm = DataModule(\n",
    "    dataset=\"ERA5\",\n",
    "    task=\"forecasting\",\n",
    "    root_dir=\"../climate-learn/data/weatherbench/era5/5.625\",\n",
    "    in_vars=[\"2m_temperature\"],\n",
    "    out_vars=[\"2m_temperature\"],\n",
    "    train_start_year=Year(1979),\n",
    "    val_start_year=Year(1980),\n",
    "    test_start_year=Year(1981),\n",
    "    end_year=Year(1982),\n",
    "    pred_range=Days(3),\n",
    "    subsample=Hours(6),\n",
    "    batch_size=128,\n",
    "    num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c4040a-19f6-46f3-921c-1a54c5a0c315",
   "metadata": {},
   "source": [
    "Create BayesianCNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d8c6d44-1c2a-4766-828b-26e2243554fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 32, 64])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2a9381-c48e-4e4d-956f-9a68d2764793",
   "metadata": {},
   "source": [
    "# Build BayesCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dfa7463-d8d9-410f-9ad0-b6b82d461899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.0 59.0\n"
     ]
    }
   ],
   "source": [
    "# Scratch cell for computing output sizes\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "hout = np.floor((29 + 2 * 0 - 1 * (3 - 1) - 1) / 1 + 1)\n",
    "wout = np.floor((61 + 2 * 0 - 1 * (3 - 1) - 1) / 1 + 1)\n",
    "\n",
    "print(hout, wout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "78e3cc64-6201-44a1-8e5f-b719c0ed763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import *\n",
    "import torch.nn as nn\n",
    "\n",
    "# https://arxiv.org/pdf/1901.02731.pdf\n",
    "\n",
    "class BayesCNN(ModuleWrapper):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = BBB_Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=3,\n",
    "            kernel_size=5,\n",
    "            stride=1,\n",
    "            padding=2\n",
    "        )\n",
    "        self.act1 = nn.Softplus()\n",
    "        self.bn1 = nn.BatchNorm2d(3)\n",
    "        self.conv2 = BBB_Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=5,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        self.act2 = nn.Softplus()\n",
    "        self.bn2 = nn.BatchNorm2d(5)\n",
    "        self.conv3 = BBB_Conv2d(\n",
    "            in_channels=5,\n",
    "            out_channels=3,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        self.act3 = nn.Softplus()\n",
    "        self.bn3 = nn.BatchNorm2d(3)\n",
    "        self.conv4 = BBB_Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=1,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "86b2d081-41ab-42f4-a567-d6ab7cb7ed01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "aed6615b-0e74-465a-818b-d0da63aad583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test it\n",
    "# outputs are (logits, kl)\n",
    "net = BayesCNN().cuda()\n",
    "logits, _ = net(dm.train_dataset[0][0].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a9baa1f9-7a47-49f0-9a8b-374ea4e870c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.1002e-02,  4.5736e-01,  8.6272e-02,  ..., -1.6405e-01,\n",
       "            4.7771e-04, -9.2713e-02],\n",
       "          [-6.0578e-01, -8.3075e-02, -1.5220e-01,  ..., -5.1696e-01,\n",
       "            5.3144e-01,  6.2323e-01],\n",
       "          [-6.2265e-01, -6.1950e-01, -1.4407e+00,  ..., -1.1370e+00,\n",
       "           -1.2253e-01, -2.7827e-01],\n",
       "          ...,\n",
       "          [-3.7115e-01,  5.3484e-01, -5.0040e-01,  ..., -8.6558e-01,\n",
       "           -4.5810e-01, -6.9343e-02],\n",
       "          [-5.9130e-01, -1.6491e-01, -6.4190e-01,  ..., -8.3902e-01,\n",
       "            3.9776e-01,  9.0848e-01],\n",
       "          [-1.4385e-01,  2.9654e-01, -4.2162e-01,  ..., -9.0225e-01,\n",
       "           -2.1108e-01, -3.7161e-01]]]], device='cuda:0',\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4dbed5e8-768e-4a91-85f4-98c2b45499f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 32, 64])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.detach().cpu().size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "47764f31-cf3a-4c17-8a2e-465d34bde31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torchvision.transforms import transforms\n",
    "from climate_learn.models.modules.utils.metrics import (\n",
    "    lat_weighted_mse,\n",
    "    lat_weighted_rmse,\n",
    "    lat_weighted_acc,\n",
    ")\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "class LitModule(pl.LightningModule):\n",
    "    def __init__(self, kl_weight):\n",
    "        super().__init__()\n",
    "        self.net = BayesCNN()\n",
    "        # function wrapper to include kl weighting\n",
    "        def wkl_loss(loss_func):\n",
    "            def wrap(*args, **kwargs):\n",
    "                kl_loss = kl_weight * kwargs.pop(\"kl\")\n",
    "                result = loss_func(*args, **kwargs)\n",
    "                key_map = {}\n",
    "                for key in result:\n",
    "                    key_map[key] = f\"wkl_{key}\"\n",
    "                for old_key, new_key in key_map.items():\n",
    "                    result[new_key] = result[old_key] + kl_loss\n",
    "                    result.pop(old_key)\n",
    "                return result\n",
    "            return wrap\n",
    "        # function wrapper to disregard kl\n",
    "        def non_wkl_loss(loss_func):\n",
    "            def wrap(*args, **kwargs):\n",
    "                if \"kl\" in kwargs:\n",
    "                    kwargs.pop(\"kl\")\n",
    "                result = loss_func(*args, **kwargs)\n",
    "                return result\n",
    "            return wrap\n",
    "        self.train_loss = [\n",
    "            non_wkl_loss(lat_weighted_mse),\n",
    "            wkl_loss(lat_weighted_mse)\n",
    "        ]\n",
    "        self.val_loss = [\n",
    "            lat_weighted_rmse,\n",
    "            lat_weighted_acc\n",
    "        ]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits, _ = self.net(torch.squeeze(x, 1))\n",
    "        return logits\n",
    "    \n",
    "    def predict(self, x):\n",
    "        logits, kl = self.net(torch.squeeze(x, 1))\n",
    "        return logits, kl\n",
    "    \n",
    "    def set_denormalization(self, mean, std):\n",
    "        self.denormalization = transforms.Normalize(mean, std)\n",
    "\n",
    "        mean_mean_denorm, mean_std_denorm = -mean / std, 1 / std\n",
    "        self.mean_denormalize = transforms.Normalize(mean_mean_denorm, mean_std_denorm)\n",
    "\n",
    "        std_mean_denorm, std_std_denorm = np.zeros_like(std), 1 / std\n",
    "        self.std_denormalize = transforms.Normalize(std_mean_denorm, std_std_denorm)\n",
    "\n",
    "        mean_mean_denorm, mean_std_denorm = -mean / std, 1 / std\n",
    "        self.mean_denormalize = transforms.Normalize(mean_mean_denorm, mean_std_denorm)\n",
    "\n",
    "        std_mean_denorm, std_std_denorm = np.zeros_like(std), 1 / std\n",
    "        self.std_denormalize = transforms.Normalize(std_mean_denorm, std_std_denorm)\n",
    "\n",
    "        mean_mean_denorm, mean_std_denorm = -mean / std, 1 / std\n",
    "        self.mean_denormalize = transforms.Normalize(mean_mean_denorm, mean_std_denorm)\n",
    "\n",
    "        std_mean_denorm, std_std_denorm = np.zeros_like(std), 1 / std\n",
    "        self.std_denormalize = transforms.Normalize(std_mean_denorm, std_std_denorm)\n",
    "\n",
    "    def set_lat_lon(self, lat, lon):\n",
    "        self.lat = lat\n",
    "        self.lon = lon\n",
    "\n",
    "    def set_pred_range(self, r):\n",
    "        self.pred_range = r\n",
    "\n",
    "    def set_train_climatology(self, clim):\n",
    "        self.train_clim = clim\n",
    "\n",
    "    def set_val_climatology(self, clim):\n",
    "        self.val_clim = clim\n",
    "\n",
    "    def set_test_climatology(self, clim):\n",
    "        self.test_clim = clim\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, _, out_variables = batch\n",
    "        y_hat, kl = self.predict(x)\n",
    "        loss_dict = [\n",
    "            m(y_hat, y, out_variables, lat=self.lat, kl=kl)\n",
    "            for m in self.train_loss\n",
    "        ][0]\n",
    "        for var in loss_dict.keys():\n",
    "            self.log(\n",
    "                \"train/\" + var,\n",
    "                loss_dict[var],\n",
    "                on_step=True,\n",
    "                on_epoch=False,\n",
    "                prog_bar=True,\n",
    "                batch_size=len(x)\n",
    "            )\n",
    "        return loss_dict\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y, variables, out_variables = batch\n",
    "        pred_steps = y.shape[1]\n",
    "        pred_range = self.pred_range.hours()\n",
    "        \n",
    "        default_days = [1, 3, 5]\n",
    "        days_each_step = pred_range / 24\n",
    "        default_steps = [\n",
    "            d / days_each_step for d in default_days if d % days_each_step == 0\n",
    "        ]\n",
    "        steps = [int(s) for s in default_steps if s <= pred_steps and s > 0]\n",
    "        days = [int(s * pred_range / 24) for s in steps]\n",
    "        day = int(days_each_step)\n",
    "                \n",
    "        preds = []\n",
    "        total_kl = 0\n",
    "        for _ in range(pred_steps):\n",
    "            x, kl = self.predict(x)\n",
    "            preds.append(x)\n",
    "            total_kl += kl\n",
    "        preds = torch.stack(preds, dim=1)\n",
    "        if len(y.shape) == 4:\n",
    "            y = y.unsqueeze(1)\n",
    "        loss_dict = [\n",
    "            m(preds, y, out_variables, transform=self.denormalization, lat=self.lat,\n",
    "              log_steps=steps, log_days=days, clim=self.val_clim)\n",
    "            for m in self.val_loss\n",
    "        ][0]\n",
    "        for var in loss_dict.keys():\n",
    "            self.log(\n",
    "                \"val/\" + var,\n",
    "                loss_dict[var],\n",
    "                on_step=True,\n",
    "                on_epoch=False,\n",
    "                prog_bar=True,\n",
    "                batch_size=len(x)\n",
    "            )\n",
    "        return loss_dict\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y, variables, out_variables = batch\n",
    "        pred_steps = y.shape[1]\n",
    "        pred_range = self.pred_range.hours()\n",
    "        day = int(pred_range / 24)\n",
    "        \n",
    "        default_days = [1, 3, 5]\n",
    "        days_each_step = pred_range / 24\n",
    "        default_steps = [\n",
    "            d / days_each_step for d in default_days if d % days_each_step == 0\n",
    "        ]\n",
    "        steps = [int(s) for s in default_steps if s <= pred_steps and s > 0]\n",
    "        days = [int(s * pred_range / 24) for s in steps]\n",
    "        day = int(days_each_step)\n",
    "        \n",
    "        # rmse for climatology baseline\n",
    "        clim_pred = self.train_clim  # C, H, W\n",
    "        clim_pred = (\n",
    "            clim_pred.unsqueeze(0)\n",
    "            .unsqueeze(0)\n",
    "            .repeat(y.shape[0], y.shape[1], 1, 1, 1)\n",
    "            .to(y.device)\n",
    "        )\n",
    "        baseline_rmse = lat_weighted_rmse(\n",
    "            clim_pred,\n",
    "            y,\n",
    "            out_variables,\n",
    "            transform_pred=False,\n",
    "            transform=self.denormalization,\n",
    "            lat=self.lat,\n",
    "            log_steps=steps,\n",
    "            log_days=days,\n",
    "        )\n",
    "        for var in baseline_rmse.keys():\n",
    "            self.log(\n",
    "                \"test_climatology_baseline/\" + var,\n",
    "                baseline_rmse[var],\n",
    "                on_step=False,\n",
    "                on_epoch=True,\n",
    "                sync_dist=True,\n",
    "                batch_size=len(x),\n",
    "            )\n",
    "\n",
    "        # rmse for persistence baseline\n",
    "        pers_pred = x  # B, 1, C, H, W\n",
    "        baseline_rmse = lat_weighted_rmse(\n",
    "            pers_pred,\n",
    "            y,\n",
    "            out_variables,\n",
    "            transform_pred=True,\n",
    "            transform=self.denormalization,\n",
    "            lat=self.lat,\n",
    "            log_steps=steps,\n",
    "            log_days=days,\n",
    "        )\n",
    "        for var in baseline_rmse.keys():\n",
    "            self.log(\n",
    "                \"test_persistence_baseline/\" + var,\n",
    "                baseline_rmse[var],\n",
    "                on_step=False,\n",
    "                on_epoch=True,\n",
    "                sync_dist=True,\n",
    "                batch_size=len(x),\n",
    "            )\n",
    "\n",
    "        # rmse for linear regression baseline\n",
    "        # check if fit_lin_reg_baseline is called by checking whether self.lr_baseline is initialized\n",
    "        try:\n",
    "            lr_pred = self.lr_baseline.predict(\n",
    "                x.cpu().reshape((x.shape[0], -1))\n",
    "            ).reshape(y.shape)\n",
    "        except AttributeError as e:\n",
    "            raise NotImplementedError(\n",
    "                \"Expect climate_learn.models.fit_lin_reg_baseline be implemented before test steps.\"\n",
    "            ) from None\n",
    "\n",
    "        lr_pred = lr_pred[:, np.newaxis, :, :, :]  # B, 1, C, H, W\n",
    "        lr_pred = torch.from_numpy(lr_pred).float().to(y.device)\n",
    "        baseline_rmse = lat_weighted_rmse(\n",
    "            lr_pred,\n",
    "            y,\n",
    "            out_variables,\n",
    "            transform_pred=True,\n",
    "            transform=self.denormalization,\n",
    "            lat=self.lat,\n",
    "            log_steps=steps,\n",
    "            log_days=days,\n",
    "        )\n",
    "        for var in baseline_rmse.keys():\n",
    "            self.log(\n",
    "                \"test_ridge_regression_baseline/\" + var,\n",
    "                baseline_rmse[var],\n",
    "                on_step=False,\n",
    "                on_epoch=True,\n",
    "                sync_dist=True,\n",
    "                batch_size=len(x),\n",
    "            )\n",
    "            \n",
    "        preds = []\n",
    "        total_kl = 0\n",
    "        for _ in range(pred_steps):\n",
    "            x, kl = self.predict(x)\n",
    "            preds.append(x)\n",
    "            total_kl += kl\n",
    "        preds = torch.stack(preds, dim=1)\n",
    "        if len(y.shape) == 4:\n",
    "            y = y.unsqueeze(1)\n",
    "        loss_dict = [\n",
    "            m(preds, y, out_variables, transform=self.denormalization, lat=self.lat,\n",
    "              log_steps=steps, log_days=days, clim=self.test_clim)\n",
    "            for m in self.val_loss\n",
    "        ][0]\n",
    "        for var in loss_dict.keys():\n",
    "            self.log(\n",
    "                \"test/\" + var,\n",
    "                loss_dict[var],\n",
    "                on_step=False,\n",
    "                on_epoch=True,\n",
    "                prog_bar=True,\n",
    "                batch_size=len(x)\n",
    "            )\n",
    "        return loss_dict\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "    \n",
    "    def fit_lin_reg_baseline(self, train_dataset, reg_hparam=0.0):\n",
    "        X_train = train_dataset.inp_data.reshape(train_dataset.inp_data.shape[0], -1)\n",
    "        y_train = train_dataset.out_data.reshape(train_dataset.out_data.shape[0], -1)\n",
    "        self.lr_baseline = Ridge(alpha=reg_hparam)\n",
    "        self.lr_baseline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d95831f2-9222-4012-9b32-a21c257bf50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LitModule(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ab2875fa-beba-468f-8374-4941e99ef51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from climate_learn.models import set_climatology\n",
    "set_climatology(lm, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3e2601db-e4d1-4d29-b44e-22122a42da22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "from climate_learn.training import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    seed=0,\n",
    "    accelerator=\"gpu\",\n",
    "    precision=16,\n",
    "    max_epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6da9d28c-37c2-4649-b826-e58e63ae0e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/home/jason.jewik/miniconda3/envs/bayes/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/jason.jewik/PyTorch-BayesianCNN/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">    </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type        </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0  </span>│ net              │ BayesCNN    │    790 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1  </span>│ net.conv1        │ BBBConv2d   │    156 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2  </span>│ net.act1         │ Softplus    │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3  </span>│ net.bn1          │ BatchNorm2d │      6 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4  </span>│ net.conv2        │ BBBConv2d   │    280 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5  </span>│ net.act2         │ Softplus    │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6  </span>│ net.bn2          │ BatchNorm2d │     10 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7  </span>│ net.conv3        │ BBBConv2d   │    276 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8  </span>│ net.act3         │ Softplus    │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9  </span>│ net.bn3          │ BatchNorm2d │      6 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 10 </span>│ net.conv4        │ BBBConv2d   │     56 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 11 </span>│ denormalization  │ Normalize   │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 12 </span>│ mean_denormalize │ Normalize   │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 13 </span>│ std_denormalize  │ Normalize   │      0 │\n",
       "└────┴──────────────────┴─────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m  \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType       \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0 \u001b[0m\u001b[2m \u001b[0m│ net              │ BayesCNN    │    790 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1 \u001b[0m\u001b[2m \u001b[0m│ net.conv1        │ BBBConv2d   │    156 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2 \u001b[0m\u001b[2m \u001b[0m│ net.act1         │ Softplus    │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3 \u001b[0m\u001b[2m \u001b[0m│ net.bn1          │ BatchNorm2d │      6 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4 \u001b[0m\u001b[2m \u001b[0m│ net.conv2        │ BBBConv2d   │    280 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m5 \u001b[0m\u001b[2m \u001b[0m│ net.act2         │ Softplus    │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m6 \u001b[0m\u001b[2m \u001b[0m│ net.bn2          │ BatchNorm2d │     10 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m7 \u001b[0m\u001b[2m \u001b[0m│ net.conv3        │ BBBConv2d   │    276 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m8 \u001b[0m\u001b[2m \u001b[0m│ net.act3         │ Softplus    │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m9 \u001b[0m\u001b[2m \u001b[0m│ net.bn3          │ BatchNorm2d │      6 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m10\u001b[0m\u001b[2m \u001b[0m│ net.conv4        │ BBBConv2d   │     56 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m11\u001b[0m\u001b[2m \u001b[0m│ denormalization  │ Normalize   │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m12\u001b[0m\u001b[2m \u001b[0m│ mean_denormalize │ Normalize   │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m13\u001b[0m\u001b[2m \u001b[0m│ std_denormalize  │ Normalize   │      0 │\n",
       "└────┴──────────────────┴─────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 790                                                                                              \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 790                                                                                                  \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 790                                                                                              \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 790                                                                                                  \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b217e35249e4260b3e3590063dfad8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(lm, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1e25e069-2f56-4590-ab0e-ac2175bc99a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.fit_lin_reg_baseline(dm.train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0219b951-013f-4389-8540-da2117c6db5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7285e705826d4a318990a2b1f5810408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">                      Test metric                       </span>┃<span style=\"font-weight: bold\">                      DataLoader 0                      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">            test/w_rmse_2m_temperature_day_3            </span>│<span style=\"color: #800080; text-decoration-color: #800080\">                   7.990107513438556                    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> test_climatology_baseline/w_rmse_2m_temperature_day_3  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">                   5.883646065011019                    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> test_persistence_baseline/w_rmse_2m_temperature_day_3  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">                   3.1876519642077183                   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> test_ridge_regression_baseline/w_rmse_2m_temperature_… </span>│<span style=\"color: #800080; text-decoration-color: #800080\">                   3.3875520106158628                   </span>│\n",
       "└────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m                     Test metric                      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                     DataLoader 0                     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m           test/w_rmse_2m_temperature_day_3           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m                  7.990107513438556                   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtest_climatology_baseline/w_rmse_2m_temperature_day_3 \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m                  5.883646065011019                   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtest_persistence_baseline/w_rmse_2m_temperature_day_3 \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m                  3.1876519642077183                  \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtest_ridge_regression_baseline/w_rmse_2m_temperature_…\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m                  3.3875520106158628                  \u001b[0m\u001b[35m \u001b[0m│\n",
       "└────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.test(lm, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69393180-8f02-456f-9e48-df3b7e13d91a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayes",
   "language": "python",
   "name": "bayes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
